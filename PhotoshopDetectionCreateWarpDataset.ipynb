{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"7QclQ3fgrPLE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671491147533,"user_tz":360,"elapsed":16865,"user":{"displayName":"Ryan Jung","userId":"09833150952226792867"}},"outputId":"6475a519-ec3c-4df5-9dbd-60cd394e52de"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Computer_Vision_Final_Project/Computer_Vision_Final_Dataset'"],"metadata":{"id":"sXmRqg6hrZjd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671491147885,"user_tz":360,"elapsed":354,"user":{"displayName":"Ryan Jung","userId":"09833150952226792867"}},"outputId":"2e86cbc4-b657-4235-9e73-39374ded1c3e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1qRPzc_kkTeNinCAOMoXj6d_nwu-u_HfH/Computer_Vision_Final_Project/Computer_Vision_Final_Dataset\n"]}]},{"cell_type":"code","source":["from __future__ import print_function, division\n","import os\n","import torch\n","import pandas as pd\n","from skimage import io, transform\n","from skimage.color import rgba2rgb\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","\n","# Ignore warnings\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","plt.ion()   # interactive mode"],"metadata":{"id":"zTpKPuS1rcLS","executionInfo":{"status":"ok","timestamp":1671491153981,"user_tz":360,"elapsed":6098,"user":{"displayName":"Ryan Jung","userId":"09833150952226792867"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["#**Helper Functions**"],"metadata":{"id":"1MbJp9ILrjGI"}},{"cell_type":"code","source":["class Rescale(object):\n","    \"\"\"Rescale the image in a sample to a given size.\n","\n","    Args:\n","        output_size (tuple or int): Desired output size. If tuple, output is\n","            matched to output_size. If int, smaller of image edges is matched\n","            to output_size keeping aspect ratio the same.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image = sample['image']\n","\n","        h, w = image.shape[:2]\n","        if isinstance(self.output_size, int):\n","            if h > w:\n","                new_h, new_w = self.output_size * h / w, self.output_size\n","            else:\n","                new_h, new_w = self.output_size, self.output_size * w / h\n","        else:\n","            new_h, new_w = self.output_size\n","\n","        new_h, new_w = int(new_h), int(new_w)\n","\n","        img = transform.resize(image, (new_h, new_w))\n","\n","        return {'image': img }\n","\n","\n","class RandomCrop(object):\n","    \"\"\"Crop randomly the image in a sample.\n","\n","    Args:\n","        output_size (tuple or int): Desired output size. If int, square crop\n","            is made.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, sample):\n","        image  = sample['image']\n","\n","        h, w = image.shape[:2]\n","        new_h, new_w = self.output_size\n","\n","        top = np.random.randint(0, h - new_h)\n","        left = np.random.randint(0, w - new_w)\n","\n","        image = image[top: top + new_h,\n","                      left: left + new_w]\n","\n","        return {'image': image }\n","\n","\n","class ToTensor(object):\n","    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n","\n","    def __call__(self, sample):\n","        image = sample['image']\n","\n","        # swap color axis because\n","        # numpy image: H x W x C\n","        # torch image: C x H x W\n","        image = image.transpose((2, 0, 1))\n","        return {'image': torch.from_numpy(image)}"],"metadata":{"id":"phvBK1uRrjlD","executionInfo":{"status":"ok","timestamp":1671491153982,"user_tz":360,"elapsed":21,"user":{"displayName":"Ryan Jung","userId":"09833150952226792867"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["#**Warping**"],"metadata":{"id":"6dACSukOrr8u"}},{"cell_type":"code","source":["%cd /content/drive/MyDrive/FALdetector-master\n","from utils.tools import *\n","from utils.visualize import *\n","from PIL import Image\n","import sys\n","%cd '/content/drive/MyDrive/Computer_Vision_Final_Project/Computer_Vision_Final_Dataset'"],"metadata":{"id":"8p4TcHxES-H7","executionInfo":{"status":"ok","timestamp":1671491156074,"user_tz":360,"elapsed":2112,"user":{"displayName":"Ryan Jung","userId":"09833150952226792867"}},"outputId":"bb47bb33-6336-43ba-c6cc-0cbd4cdb6ff1","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/FALdetector-master\n","/content/drive/.shortcut-targets-by-id/1qRPzc_kkTeNinCAOMoXj6d_nwu-u_HfH/Computer_Vision_Final_Project/Computer_Vision_Final_Dataset\n"]}]},{"cell_type":"code","source":["class WarpDataset(Dataset):\n","    \"\"\"Photoshop Detection dataset.\"\"\"\n","\n","    def __init__(self, csv_file, root_dir, transform=None):\n","        \"\"\"\n","        Args:\n","            csv_file (string): Path to the csv file with annotations.\n","            root_dir (string): Directory with all the images.\n","            transform (callable, optional): Optional transform to be applied\n","                on a sample.\n","        \"\"\"\n","        self.warp_dataset = pd.read_csv(csv_file)\n","        self.root_dir = root_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.warp_dataset)\n","\n","    def __getitem__(self, idx, find_face=False):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        \n","        img_path = os.path.join(self.root_dir,\n","                                self.warp_dataset.iloc[idx, 0])\n","        image = io.imread(img_path)\n","        if image.shape[2] == 4:\n","          image = rgba2rgb(image)\n","        im_w, im_h = Image.open(img_path).size\n","        if find_face:\n","          faces = face_detection(img_path, verbose=False, model_file='/content/drive/MyDrive/FALdetector-master/utils/dlib_face_detector/mmod_human_face_detector.dat')\n","          if len(faces) == 0:\n","              print(\"no face detected by dlib, exiting\")\n","              sys.exit()\n","          face, box = faces[0]\n","          image = resize_shorter_side(face, 400)[0]\n","\n","        labels = self.warp_dataset.iloc[idx, 2]\n","        labels = np.array([labels])\n","        labels = labels.astype('float')\n","        sample = {'image': image, 'labels': labels}\n","\n","        if self.transform:\n","          sample = self.transform(sample)\n","\n","        return sample['image'], labels"],"metadata":{"id":"ANc7n3kmrrqJ","executionInfo":{"status":"ok","timestamp":1671491156075,"user_tz":360,"elapsed":5,"user":{"displayName":"Ryan Jung","userId":"09833150952226792867"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["warp_dataset = WarpDataset(csv_file='PhotoshopDetectionDataset.csv', \n","                           root_dir='./Collective',\n","                           transform=transforms.Compose([\n","                                               Rescale(512),\n","                                               RandomCrop(224),\n","                                               ToTensor()\n","                                        ]))"],"metadata":{"id":"UgGl1pRbruhm","executionInfo":{"status":"ok","timestamp":1671491156360,"user_tz":360,"elapsed":289,"user":{"displayName":"Ryan Jung","userId":"09833150952226792867"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"drwXEzC1Ip36","executionInfo":{"status":"ok","timestamp":1671491156361,"user_tz":360,"elapsed":6,"user":{"displayName":"Ryan Jung","userId":"09833150952226792867"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["%cd .."],"metadata":{"id":"oAdY4y53rvmm","executionInfo":{"status":"ok","timestamp":1671491156361,"user_tz":360,"elapsed":5,"user":{"displayName":"Ryan Jung","userId":"09833150952226792867"}},"outputId":"cff283ba-a4c5-4140-d8fc-fbf8505f55d5","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1qRPzc_kkTeNinCAOMoXj6d_nwu-u_HfH/Computer_Vision_Final_Project\n"]}]}]}